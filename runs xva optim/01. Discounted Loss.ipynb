{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NB_SIM = 1\n",
    "QUANTILES = [0.95, 0.99, 0.997]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rec_dict_print(a_dict, level=1):\n",
    "    keys = a_dict.keys()\n",
    "    keys.sort()\n",
    "    for k in keys:\n",
    "        if type(a_dict[k]) is dict:\n",
    "            print \"\".join([\"-\" for i in range(level)]) + k\n",
    "            rec_dict_print(a_dict[k], level + 2)\n",
    "        else:\n",
    "            print \"\".join([\"-\" for i in range(level)]) + k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import dill\n",
    "\n",
    "loaded_data = None\n",
    "\n",
    "with open('precomputed_sims/data%i.pkl' % (NB_SIM), 'rb') as f:\n",
    "    loaded_data = dill.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rec_dict_print(loaded_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N = loaded_data[\"N\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Instead of taking 365 standard days or 252 trading days\n",
    "# in order to get some easy computations for the eqty and df time grids\n",
    "# I chose to take 360 days of tradings\n",
    "\n",
    "step = 1 / 360.\n",
    "delta = 5 * step\n",
    "\n",
    "maturity = loaded_data[\"randomization\"][\"maturity\"]\n",
    "\n",
    "print \"Maturity = %s years\" % maturity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from finance.discountfactor import ConstantRateDiscountFactor \n",
    "\n",
    "r = 0.02\n",
    "discount = ConstantRateDiscountFactor(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Underlying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "udlyings = loaded_data[\"underlyings\"]\n",
    "\n",
    "print \"Maximum number of paths: %i\" % len(udlyings)\n",
    "\n",
    "gbm0 = udlyings[0]\n",
    "\n",
    "kappa = gbm0.drifts[0][0]\n",
    "sigma = gbm0.vols[0][0]\n",
    "print \"kappa = %s, sigma = %s\" % (kappa, sigma)\n",
    "\n",
    "time_grid = gbm0.time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Derivative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "derivatives_nb = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from finance.products.european.swap import (\n",
    "    SwapContract,\n",
    ")\n",
    "\n",
    "swap_delta = 0.25\n",
    "\n",
    "swap_dates = SwapContract.generate_payment_dates(0, maturity, swap_delta)\n",
    "swap = SwapContract(gbm0, discount, swap_dates)\n",
    "\n",
    "price_0 = swap.price(0., incl_next_coupon=True)\n",
    "\n",
    "print swap\n",
    "print \"\\nPrice swap at t=0 without 1st coupon = %s\" % price_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "p_fixed = 1.\n",
    "strike = swap.strike\n",
    "\n",
    "delta_times = swap.delta_time\n",
    "discount_factors = [discount(t) for t in swap.pillars[1:]]\n",
    "\n",
    "delta_beta_sum = np.dot(delta_times, discount_factors)\n",
    "\n",
    "notional = p_fixed / (strike*delta_beta_sum)\n",
    "\n",
    "print \"Notional on the swap: %s\" % notional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Indexes stuffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "copula = loaded_data[\"credit\"][\"copula\"]\n",
    "\n",
    "c_subsets_indexes = loaded_data[\"credit\"][\"bc_subsets_indexes\"]\n",
    "\n",
    "obligors_nb = len(copula.subsets[c_subsets_indexes[-1]][0])\n",
    "print \"Obligor numbers: %s\" % obligors_nb\n",
    "\n",
    "c_ids = [17, 9, 29, 26, 50, 4, 5, 13, 64]\n",
    "c_positions = [0.69, -0.46, -0.44, -0.36, 0.34, 0.23, 0.09, -0.05, -0.04]\n",
    "\n",
    "print \"Counterparties id: %s (nb = %s)\" % (c_ids, len(c_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "positions = np.zeros(obligors_nb)\n",
    "for idx, ps in zip(c_ids, c_positions):\n",
    "    positions[idx] = ps\n",
    "\n",
    "positions = positions / -positions[13]\n",
    "positions = np.array(positions).reshape(positions.size, 1)\n",
    "\n",
    "print positions.flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Default times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "default_times_mat = loaded_data[\"credit\"][\"default_times\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def get_default_times_sim(c_ids_, N_, default_times_mat_, copula_, subsets_indexes_):\n",
    "    matrix_def_ = np.zeros((len(c_ids_), N_))\n",
    "    \n",
    "    for j_, c_id_ in enumerate(c_ids_):\n",
    "        cp_subsets_indexes_ = copula_.get_indexes_including(c_id_)\n",
    "        c_df_times_indexes_ = [ii_ for ii_, ind_ in enumerate(subsets_indexes_) if ind_ in cp_subsets_indexes_]\n",
    "        \n",
    "        c_df_times_mat_ = default_times_mat_[:, c_df_times_indexes_]\n",
    "        c_df_times_ = c_df_times_mat_.min(axis=1)\n",
    "        \n",
    "        matrix_def_[j_] = c_df_times_\n",
    "        \n",
    "    matrix_def_[matrix_def_==1000.] = np.nan\n",
    "        \n",
    "    df_default_times_ = pd.DataFrame(matrix_def_, index=np.array(c_ids_))\n",
    "    return df_default_times_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "default_times = get_default_times_sim(c_ids, N, default_times_mat, copula, c_subsets_indexes)\n",
    "default_times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Portfolio P&L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code for:\n",
    "\n",
    "\\begin{equation*}\n",
    "% \\sum_{t < \\tau_i^{\\delta} \\leq t+1} \n",
    "\\left( \n",
    "\\beta_{\\tau_i^{\\delta}} \\left( MtM_{\\tau_i^\\delta}^i + \\Delta^i_{\\tau_i^\\delta} \\right)\n",
    "-\\beta_{\\tau_i} \\left( {\\rm VM}_{\\tau_i}^i + {\\rm IM}_{\\tau_i}^i \\right)\n",
    "\\right)^+ \\, \\forall i\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from utils import time_offseter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "times_cva = np.arange(0, maturity, 0.5)\n",
    "\n",
    "shifted_times_cva = times_cva + 1.\n",
    "shifted_times_cva[-1] = maturity\n",
    "\n",
    "default_times.index.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "def fact_(swap_, discount_, kappa_, delta_, t_):\n",
    "    time_grid_ = swap_.underlying.time\n",
    "    t_delta_ = time_offseter(t_ + delta_, time_grid_, True)\n",
    "    \n",
    "    coupon_dates_ = swap_.pillars\n",
    "    l_t_delta_ = np.searchsorted(coupon_dates_, t_delta_, side='left')\n",
    "\n",
    "    beta_T_l_ = map(discount_, coupon_dates_[l_t_delta_ + 1 :])    \n",
    "    h_l_ = swap_.delta_time[l_t_delta_ : ]\n",
    "    \n",
    "    T_l_m1_ = kappa_ * coupon_dates_[l_t_delta_ : -1]\n",
    "    exp_factor_ = map(np.exp, T_l_m1_)\n",
    "    \n",
    "    tmp_ = np.multiply(exp_factor_, h_l_)    \n",
    "    res_ = np.dot(beta_T_l_, tmp_)\n",
    "    \n",
    "    return res_\n",
    "\n",
    "fact = partial(fact_, swap, discount, kappa, delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_1y_ahead_loss_(fact_f_, kappa_, delta_, notional_, positions_, times_cva_, shifted_times_cva_, default_times_, udls_):\n",
    "    ccp_loss_1y_ = dict()\n",
    "    \n",
    "    N_ = len(udls_)\n",
    "    \n",
    "    c_ids_ = default_times_.index.values\n",
    "    \n",
    "    for t0_, t1_ in zip(times_cva_, shifted_times_cva_):\n",
    "        key_ = \"[%.2f, %.2f]\" % (t0_, t1_)\n",
    "        print key_\n",
    "        ccp_loss_1y_[key_] = pd.DataFrame(0., index=range(N_), columns=c_ids_)\n",
    "        \n",
    "        for j_ in xrange(N_):\n",
    "            udl_ = udls_[j_]\n",
    "            time_grid_ = udl_.time\n",
    "            tau_j_ = default_times_.loc[c_ids_, j_]\n",
    "            \n",
    "            c_defaulted_ = tau_j_[(tau_j_ > t0_) & (tau_j_ <= t1_)]\n",
    "            for (c_idx_, tau_) in c_defaulted_.iteritems():\n",
    "                tau_ = time_offseter(tau_, time_grid_, True)\n",
    "                tau_delta_ = time_offseter(tau_ + delta_, time_grid_, True)\n",
    "                \n",
    "                hat_s_tau_ = udl_(tau_)[0][0] * np.exp(kappa_ * tau_)\n",
    "                hat_s_tau_delta_ = udl_(tau_delta_)[0][0] * np.exp(kappa_ * tau_delta_)\n",
    "            \n",
    "                loss_c_ = notional_ * fact_f_(tau_) * positions_[c_idx_][0] * (hat_s_tau_ - hat_s_tau_delta_)\n",
    "                \n",
    "                ccp_loss_1y_[key_].loc[j_, c_idx_] = loss_c_\n",
    "\n",
    "    return ccp_loss_1y_\n",
    "            \n",
    "compute_1y_ahead_loss = partial(compute_1y_ahead_loss_, fact, kappa, delta, notional, positions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "loss_path = './res/sim%i/loss_1y_ahead/' % (NB_SIM)\n",
    "\n",
    "if not os.path.isdir(loss_path):\n",
    "    os.makedirs(loss_path)\n",
    "    one_y_ahead_loss = compute_1y_ahead_loss(times_cva, shifted_times_cva, default_times, udlyings)\n",
    "    \n",
    "    for k, dataframe_ in one_y_ahead_loss.iteritems():\n",
    "        path = os.path.join(loss_path, 'loss_no_im%s.csv' % k)\n",
    "        dataframe_.to_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "\n",
    "def B_(fact_f_, vol_, delta_, a_, omega_, t_):\n",
    "    if a_ <= 0.5: \n",
    "        a_ = 1. - a_\n",
    "    \n",
    "    perc_ = a_ if omega_ <= 0. else (1. - a_)\n",
    "    q_ = norm.ppf(perc_)\n",
    "    \n",
    "    var_ = vol_**2 * delta_\n",
    "    exp_factor_ = np.exp(-0.5 * var_ + np.sqrt(var_) * q_)\n",
    "    \n",
    "    tmp_res_ = 1. - exp_factor_\n",
    "    \n",
    "    return np.sign(omega_) * tmp_res_ * fact_f_(t_)\n",
    "\n",
    "B = partial(B_, fact, sigma, delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_disc_margins_(b_f_, kappa_, notional_, positions_, times_cva_, shifted_times_cva_, default_times_, udls_, q_):\n",
    "    margins_ = dict()\n",
    "    \n",
    "    N_ = len(udls_)\n",
    "    \n",
    "    c_ids_ = default_times_.index.values\n",
    "    \n",
    "    print q_\n",
    "    for t0_, t1_ in zip(times_cva_, shifted_times_cva_):\n",
    "        key_ = \"[%.2f, %.2f]\" % (t0_, t1_)\n",
    "        print key_\n",
    "        margins_[key_] = pd.DataFrame(0., index=range(N_), columns=c_ids_)\n",
    "        \n",
    "        for j_ in xrange(N_):\n",
    "            udl_ = udls_[j_]\n",
    "            time_grid_ = udl_.time\n",
    "            tau_j_ = default_times_.loc[c_ids_, j_]\n",
    "            \n",
    "            c_defaulted_ = tau_j_[(tau_j_ > t0_) & (tau_j_ <= t1_)]\n",
    "            for (c_idx_, tau_) in c_defaulted_.iteritems():\n",
    "                tau_ = time_offseter(tau_, time_grid_, True)\n",
    "                                \n",
    "                hat_s_tau_ = udl_(tau_)[0][0] * np.exp(kappa_ * tau_)\n",
    "                \n",
    "                nom_i = notional_ * np.abs(positions_[c_idx_][0])\n",
    "                margin_ = nom_i * hat_s_tau_ * b_f_(q_, positions_[c_idx_][0], tau_)\n",
    "            \n",
    "                margins_[key_].loc[j_, c_idx_] = margin_\n",
    "\n",
    "    return margins_\n",
    "\n",
    "compute_disc_margins = partial(compute_disc_margins_, B, kappa, notional, positions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "im_path = './res/sim%i/im/' % (NB_SIM)\n",
    "\n",
    "for q in QUANTILES:\n",
    "    im_q_path = os.path.join(im_path, str(q))\n",
    "    if not os.path.isdir(im_q_path):\n",
    "        os.makedirs(im_q_path)\n",
    "        \n",
    "    q_margins = compute_disc_margins(times_cva, shifted_times_cva, default_times, udlyings, q)\n",
    "    for k, dataframe_ in q_margins.iteritems():\n",
    "        path = os.path.join(im_q_path, 'im%s.csv' % k)\n",
    "        dataframe_.to_csv(path)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
